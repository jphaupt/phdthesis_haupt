\chapter{Monte Carlo Methods}
\label{chap:qmc}
\todo{...}
\todo{look especially at thijssen and the foulkes review for these sections}

% probably get some inspiration from Kai's and Werner's theses -- focus on FCIQMC

\gls{MC} methods are a class of numerical methods that use random sampling to numerically solve problems. It has found applications in an impressive range of fields, from physics to finance.\todo{citation} It is particularly useful for problems with high dimensionality, where deterministic methods are often impractical. In quantum chemistry and physics, since a `dimension' can refer to any degree of freedom, high-dimensional problems are commonplace, and so \gls{MC} methods are a natural choice.

While the name \gls{MC} was coined by Stanislav Ulam, after the famous casino in Monaco,\todo{citation} the foundational concept was already developed in the 18th century by the French mathematician Georges-Louis Leclerc, Comte de Buffon. As one of the earliest example applications, in the Buffon needle problem, one can randomly toss needles onto a lined sheet of paper and determine $\pi$.\todo{citation}

\section{Classical Monte Carlo Methods}

We start our discussion with classical \gls{MC} methods. We consider the classical textbook problem of calculating the value of $\pi$, then we provide a more rigorous framework.

\subsection{A Very Bad Game of Darts}

If we imagine throwing darts at a dartboard randomly, we can approximate $\pi$. If the radius of the circle is $r$, then its area is $\pi r^2$. The area of the square circumscribing the circle is $4r^2$. Therefore, the ratio of the area of the circle to the area of the square is $\pi/4$. If we randomy sample a point in the square (``throw a dart''), the probability that the point is inside the circle is proportional its area. Since we sample inside the square, the probability of landing inside the circle is
\begin{equation}
    P(\text{inside circle}) =  \frac{\pi r^2}{4r^2} = \frac{\pi}{4}.
\end{equation}
Therefore, if we sample a large number of points, the ratio of the number of darts that land inside the unit circle to the total number of darts, we can approximate the probability distribution $P$ and thus get an estimate for $\pi$. This is illustrated in figure \ref{fig:darts}, and captures the core philosophy of \gls{MC} methods.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/qmc/darts.pdf}
    \caption{Our ``game of darts''. Points inside the unit circle are coloured blue whereas points outside are orange. Using stochastic sampling, this naive approach uses 5000 randomly generate numbers between -1 and 1 to approximate $\pi\approx 4N_\mathrm{in}/N_\mathrm{out}\approx 3.1464$. Of course, there are many ways to improve this method, the most obvious being to use a fraction of the unit circle, such as the first quadrant.}
    \label{fig:darts}
\end{figure}

\subsection{A More Mathematical Description}

As we have expressed the problem of the previous section in terms of areas, it is clear that it can also be formulated in terms of integrals. For this particular problem, we have:
\begin{equation}
    \pi = \int_{-1}^1 \int_{-1}^1 \mathrm{d}x \mathrm{d}y \Theta(1-x^2-y^2) .
\end{equation}

More generally, consider the integral of some smooth function $f$ over $[a,b]\subseteq \mathbb{R}$,
\begin{equation}
    I = \int_a^b \d x f(x).
\end{equation}
Standard finite element methods for solving integrals of this type typically involving dividing the integration domain into subintervals of length $h$ and determining the weights $w_i$ from e.g. a polynomial approximation. The error $\sigma$ in these sorts of methods is typically $\sigma\propto h^{-k} \propto N^{-k}$, where $k\in\mathbb{Z}_{>0}$. For a multi-dimensional integral, $\sigma\propto N^{-k/d}$, where $d$ is the number of dimensions.\cite{ascherFirst2011}
% I am mostly following the discussion and notation in Thijssen's book -- remember to cite it along with all the primary references

In \gls{MC} methods,\todo{citations} $\forall i$ take $w_i=1$ and $x_i\in[a,b]$ randomly sampled. For uniformly sampled points, t`he variance is then
\begin{align}
    \sigma^2 &= \left\langle \left( \frac{b-a}{N}\sum^N_{i=1}f_i\right)2\right\rangle - \left( \left\langle  \frac{b-a}{N}\sum^N_{i=1}f_i\right\rangle\right)^2 \\
    &= \frac{(b-a)^2}{N}(\bar{f^2} - \bar f^2)
\end{align}
where $f_i\mathdef f(x_i)$ where $x_i$ is a random number drawn, the angular brackets denote an average over all possible realisations, and the overbar represents an average of the function over the domain ($[a,b]$). i.e. the error in this method is proportional to the variance of $f$. Perhaps more interestingly, $\sigma\propto N^{-1/2}$, in line with the central limit theorem.\todo{citations}

\todo{complete discussion with some basic importance sampling stuff}


\section{Variational (Quantum) Monte Carlo}
\label{sec:vmc}
\todo{make sure to mention the Jastrow factor}

\section{Diffusion Monte Carlo}
\todo{main point here is just its similarity to FCIQMC}

\section{Auxiliary Field Quantum Monte Carlo}

\section{The Fermion Sign Problem}
\todo{make sure to mention the fixed node approximation here}

\section{QMC Meets Electronic Structure: the FCIQMC Algorithm}
\label{sec:fciqmc}

\subsection{Main Concepts}

\subsection{Energy Estimators}

\subsection{The Sign Problem in FCIQMC}

\subsection{The Initiator Approximation}

\subsection{Reduced Density Matrix Sampling}

\subsection{Combining TC with Modern Electronic Structure}
