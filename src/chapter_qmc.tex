\chapter{Monte Carlo Methods}
\label{chap:qmc}
\todo{...}
\todo{look especially at thijssen and the foulkes review for these sections}

% probably get some inspiration from Kai's and Werner's theses -- focus on FCIQMC

\gls{MC} methods are a class of numerical methods that use random sampling to numerically solve problems. It has found applications in an impressive range of fields, from physics to finance.\todo{citation} It is particularly useful for problems with high dimensionality, where deterministic methods are often impractical. In quantum chemistry and physics, since a `dimension' can refer to any degree of freedom, high-dimensional problems are commonplace, and so \gls{MC} methods are a natural choice.

While the name \gls{MC} was coined by Stanislav Ulam, after the famous casino in Monaco,\todo{citation} the foundational concept was already developed in the 18th century by the French mathematician Georges-Louis Leclerc, Comte de Buffon. As one of the earliest example applications, in the Buffon needle problem, one can randomly toss needles onto a lined sheet of paper and determine $\pi$.\todo{citation}

\section{Classical Monte Carlo Methods}

We start our discussion with classical \gls{MC} methods. We consider the classical textbook problem of calculating the value of $\pi$, then we provide a more rigorous framework.

\subsection{A Very Bad Game of Darts}

If we imagine throwing darts at a dartboard randomly, we can approximate $\pi$. If the radius of the circle is $r$, then its area is $\pi r^2$. The area of the square circumscribing the circle is $4r^2$. Therefore, the ratio of the area of the circle to the area of the square is $\pi/4$. If we randomy sample a point in the square (``throw a dart''), the probability that the point is inside the circle is proportional its area. Since we sample inside the square, the probability of landing inside the circle is
\begin{equation}
    P(\text{inside circle}) =  \frac{\pi r^2}{4r^2} = \frac{\pi}{4}.
\end{equation}
Therefore, if we sample a large number of points, the ratio of the number of darts that land inside the unit circle to the total number of darts, we can approximate the probability distribution $P$ and thus get an estimate for $\pi$. This is illustrated in figure \ref{fig:darts}, and captures the core philosophy of \gls{MC} methods.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/qmc/darts.pdf}
    \caption{Our ``game of darts''. Points inside the unit circle are coloured blue whereas points outside are orange. Using stochastic sampling, this naive approach uses 5000 randomly generate numbers between -1 and 1 to approximate $\pi\approx 4N_\mathrm{in}/N_\mathrm{out}\approx 3.1464$. Of course, there are many ways to improve this method, the most obvious being to use a fraction of the unit circle, such as the first quadrant.}
    \label{fig:darts}
\end{figure}

\subsection{A More Mathematical Description}

As we have expressed the problem of the previous section in terms of areas, it is clear that it can also be formulated in terms of integrals. For this particular problem, we have:
\begin{equation}
    \pi = \int_{-1}^1 \int_{-1}^1 \mathrm{d}x \mathrm{d}y \Theta(1-x^2-y^2) .
\end{equation}

More generally, consider the integral of some smooth function $f$ over $[a,b]\subseteq \mathbb{R}$,\footnote{We present the one-dimensional case for simplicity. The generalization to higher dimensions is straightforward.}
\begin{equation}
    I = \int_a^b \d x f(x).
\end{equation}
Standard finite element methods for solving integrals of this type typically involving dividing the integration domain into subintervals of length $h$ and determining the weights $w_i$ from e.g. a polynomial approximation. The error $\sigma$ in these sorts of methods is typically $\sigma\propto h^{-k} \propto N^{-k}$, where $k\in\mathbb{Z}_{>0}$. For a multi-dimensional integral, $\sigma\propto N^{-k/d}$, where $d$ is the number of dimensions.\cite{ascherFirst2011}
% I am mostly following the discussion and notation in Thijssen's book -- remember to cite it along with all the primary references

In \gls{MC} methods,\todo{citations} $\forall i$ take $w_i=1$ and $x_i\in[a,b]$ randomly sampled. For uniformly sampled points, t`he variance is then
\begin{align}
    \sigma^2 &= \left\langle \left( \frac{b-a}{N}\sum^N_{i=1}f_i\right)2\right\rangle - \left( \left\langle  \frac{b-a}{N}\sum^N_{i=1}f_i\right\rangle\right)^2 \\
    &= \frac{(b-a)^2}{N}(\bar{f^2} - \bar f^2)
\end{align}
where $f_i\mathdef f(x_i)$ where $x_i$ is a random number drawn, the angular brackets denote an average over all possible realisations, and the overbar represents an average of the function over the domain ($[a,b]$ in this discussion). i.e. the error in this method is proportional to the variance of $f$. Perhaps more interestingly, $\sigma\propto N^{-1/2}$, in line with the central limit theorem.\todo{citations} Comparing this error with standard quadrature, we see that MC integration is moroe efficient than an order-$k$ algorithm when $d>2k$. That is, although this particular \gls{MC} example is naive, using simply a uniform distribution, it is still more efficient than standard methodologies for very large dimensions.

There exist several methods to reduce errors in \gls{MC} methods.\todo{citations} Among the most important ones is importance sampling. In the prevous example, it is clear that if significant contributions to the integral come from a small region of the domain, only a few points would be sampled by the \gls{MC} algorithm there. This would lead to large statistical errors. Mathematically, this is from the large variance of the function. In importance sampling, we sample from a distribution $p$ which has roughly the same shape as $f$ such that $f/p$ is roughly constant over the integration domain. Of course, being a probability distribution, we require
\begin{equation}
    p(x)>0 \quad \forall x
\end{equation}
and
\begin{equation}
    \int\d x \ p(x) = 1.
\end{equation}

Then, the integral is
\begin{equation}
    I = \int\d x \  f(x) = \int\d x\ \frac{f(x)}{p(x)} p(x).
\end{equation}
Then, if we sample points according to $p$, we have
\begin{equation}
    I\approx \frac{1}{N}\sum^N_{i=1} \frac{f(x_i)}{p(x_i)},
\end{equation}
where the naive \gls{MC} method is recovered when $p(x)=1/(b-a)$, i.e. the uniform distribution.

In this case, the variance in the result is
\begin{equation}
    \sigma^2 = \left\langle \left( \frac{1}{N}\sum^N_{i=1} \frac{f(x_i)}{p(x_i)}\right)^2\right\rangle - \left( \left\langle  \frac{1}{N}\sum^N_{i=1} \frac{f(x_i)}{p(x_i)}\right\rangle\right)^2.
\end{equation}
From this, we can see if $f/p$ is constant, the error vanishes, so it is important to choose a good $p$. If $p$ is chosen poorly, the variance can be worsened, so this is a delicate problem.

In practice it is often difficult to estimate $f$, and a choice for $p$ is problem-specific. However, when we have some approximation for its overall shape, importance sampling can be a powerful tool. Alternatively, another method known as adaptive Monte Carlo\todo{citation} seeks the most significant regions of $f$ by random sampling, so that no a priori knowledge of the functional form is required.

\section{Variational (Quantum) Monte Carlo}
\label{sec:vmc}
\todo{make sure to mention the Jastrow factor}

\section{Diffusion Monte Carlo}
\todo{main point here is just its similarity to FCIQMC}

\section{Auxiliary Field Quantum Monte Carlo}

\section{The Fermion Sign Problem}
\todo{make sure to mention the fixed node approximation here}

\section{QMC Meets Electronic Structure: the FCIQMC Algorithm}
\label{sec:fciqmc}

\subsection{Main Concepts}

\subsection{Energy Estimators}

\subsection{The Sign Problem in FCIQMC}

\subsection{The Initiator Approximation}

\subsection{Reduced Density Matrix Sampling}

\subsection{Combining TC with Modern Electronic Structure}
